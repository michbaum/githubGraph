{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from top_github_scraper import (get_top_repo_urls, get_top_repos, get_top_contributors, \n",
    "get_top_user_urls, get_top_users)\n",
    "#import datapane as dp \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "USERNAME = os.getenv(\"GITHUB_USERNAME\")\n",
    "TOKEN = os.getenv(\"GITHUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769a630",
   "metadata": {},
   "source": [
    "### Define your search keywords\n",
    "\n",
    "We are scraping 38 curated 'github_topics' to get a good and diverse selection of GitHub repositories.\n",
    "Feel free to change these topics for your own interests/needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"data science\",\"api\"]\n",
    "github_topics = ['3D','Algorithm','Android','API','Arduino','Atom','aws','azure','bash','bootstrap','chrome','compiler','crytocurrency','data structures','database','data visualization','deep learning','data science','deployment','flask','front end','git','google','iOS','json','library','machine learning','macOS','mobile','modeling','natural language processing','neural network','operating system','parsing','software','server','virtual reality','windows']\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553ac76",
   "metadata": {},
   "source": [
    "#### Scraping functions\n",
    "\n",
    "##### GitHub Repo Scraping Functions\n",
    "In the following cell we define the GitHub repository scraping functions. The basis for these were given in the following [Medium article](https://towardsdatascience.com/i-scraped-more-than-1k-top-machine-learning-github-profiles-and-this-is-what-i-found-1ab4fb0c0474)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include the start parameter, also in the all_repo function!\n",
    "def get_repo_info(keyword, start=0, stop=10):\n",
    "    \"\"\"\n",
    "    Scrapes important information from the first 'stop' pages of GitHub repositories queried with 'keyword'.\n",
    "  \n",
    "    Grabs 'name',\"stargazers_count\", \"forks_count\", 'subscribers_count', 'topics', 'language', 'created_at','updated_at' information\n",
    "    from all of these repositories and adds the 'url' as well as the 'search_word' to it. \n",
    "  \n",
    "    Parameters:\n",
    "    keyword (string): Keyword to search GitHub repositories for\n",
    "    start (int): First page of the query to be taken. Can be used to make subsequent, smaller query calls. Default = 0.\n",
    "    stop (int): Limits the amount of pages to be scraped by the query. Default = 10.\n",
    "  \n",
    "    Returns:\n",
    "    dataframe: A dataframe combining all the scraped information for the relevant repositories.\n",
    "  \n",
    "    \"\"\"\n",
    "    # First gather the first 'stop' pages of GitHub repos associated with that keyword\n",
    "    repos = get_repo_urls(keyword, start=start, stop=stop)\n",
    "    \n",
    "    all_repo_info = dict()\n",
    "    # Information to be scraped from every repo\n",
    "    info_to_scrape = ['name', 'stargazers_count', 'forks_count', 'subscribers_count', 'topics', 'language', 'created_at', 'updated_at']\n",
    "    for repo in tqdm(repos,desc=\"Scraping top repo info...\"):\n",
    "        repo_url = repo\n",
    "        repo_info_url = f\"https://api.github.com/repos{repo_url}\"\n",
    "        # Issue an API request\n",
    "        repo_info = requests.get(repo_info_url, auth=(USERNAME, TOKEN))\n",
    "        # Check if too many requests have been sent, wait a bit and try again\n",
    "        while repo_info.status_code == 429:\n",
    "            print(\"Timeout, retrying to fetch repository information...\")\n",
    "            time.sleep(30)\n",
    "            repo_info = requests.get(repo_info_url, auth=(USERNAME, TOKEN))\n",
    "        repo_info = repo_info.json()\n",
    "        repo_name = repo_info['id']\n",
    "        repo_important_info = {}\n",
    "        for info in info_to_scrape:\n",
    "            repo_important_info[info] = repo_info[info]\n",
    "        repo_important_info['url'] = repo_url\n",
    "        repo_important_info['search_word'] = keyword\n",
    "        all_repo_info[repo_name] = repo_important_info\n",
    "    # Build a dataframe out of the scraped data\n",
    "    repo_df = pd.DataFrame.from_dict(all_repo_info, orient='index', columns=info_to_scrape+['url','search_word'])\n",
    "    return repo_df\n",
    "\n",
    "def all_repo_info(keywords, start = 0, stop=10):\n",
    "    \"\"\"\n",
    "    Scrapes important information from the first 'stop' pages of all GitHub repositories queried with all keywords in 'keywords'.\n",
    "    Writes the scraped data to disk as \"most_updated_repo_info_stop'start'to'stop'.csv\"\n",
    "  \n",
    "    Grabs 'name',\"stargazers_count\", \"forks_count\", 'subscribers_count', 'topics', 'language', 'created_at','updated_at' information\n",
    "    from all of these repositories and adds the 'url' as well as the 'search_word' to it.\n",
    "  \n",
    "    Parameters:\n",
    "    keywords (List[string]): Keywords to search GitHub repositories for\n",
    "    start (int): First page of the query to be taken. Can be used to make subsequent, smaller query calls. Default = 0.\n",
    "    stop (int): Limits the amount of pages to be scraped by the query. Default = 10.\n",
    "  \n",
    "    Returns:\n",
    "    dataframe: A dataframe combining all the scraped information for the relevant repositories.\n",
    "  \n",
    "    \"\"\"\n",
    "    repo_df = pd.DataFrame(columns=['name',\"stargazers_count\", \"forks_count\", 'subscribers_count', 'topics', 'language', 'created_at','updated_at','url','search_word'])\n",
    "    for k in keywords:\n",
    "        # Gather all important information about the repos associated with a specific keyword\n",
    "        new_repo = get_repo_info(k, start=start, stop=stop)\n",
    "        print(k,len(new_repo.index))\n",
    "        # Combine dataframes for different keywords\n",
    "        repo_df = pd.concat([repo_df,new_repo])\n",
    "        # Write partial result to disk to keep progress\n",
    "        repo_df.to_csv(f'data/most_updated_repo_info_stop{start}to{stop}.csv')\n",
    "    return repo_df\n",
    "            \n",
    "\n",
    "def topic_relationship_table(repo_df, output_file):\n",
    "    \"\"\"\n",
    "    Builds a GitHub repository id to repo topic relationship table and returns it.\n",
    "  \n",
    "    Parameters:\n",
    "    repo_df (Dataframe): GitHub repo dataframe to build id-topic-relationship table for\n",
    "  \n",
    "    Returns:\n",
    "    dataframe: A new dataframe containing a separate mapping from a repo ID to all its related topics.\n",
    "  \n",
    "    \"\"\"\n",
    "    id_list = []\n",
    "    topic_list = []\n",
    "    for i in repo_df.index:\n",
    "        topics = repo_df.loc[i,'topics']\n",
    "        for t in topics:\n",
    "            # Build the individual pairing\n",
    "            id_list.append(i)\n",
    "            topic_list.append(t)\n",
    "    df = pd.DataFrame({'id':id_list,'topic':topic_list})\n",
    "    df.to_csv(output_file)\n",
    "    return df\n",
    "\n",
    "\n",
    "SCRAPE_CLASS = {'Users': 'mr-1', 'Repositories': \"v-align-middle\"}\n",
    "TYPE = 'Repositories'\n",
    "def get_repo_urls(keyword, start=0, stop=10):\n",
    "    \"\"\"\n",
    "    Queries the 'start' until 'stop' pages of GitHub repositories associated with the given keyword and returns their URLs as a list.\n",
    "  \n",
    "    Also takes into consideration API timeouts and subsequently waits for 60 seconds upon such a timeout, so it may\n",
    "    take a while.\n",
    "  \n",
    "    Parameters:\n",
    "    keyword (string): Keyword to search GitHub repositories for\n",
    "    start (int): First page of the query to be taken. Can be used to make subsequent, smaller query calls. Default = 0.\n",
    "    stop (int): Limits the amount of pages to be scraped by the query. Default = 10.\n",
    "  \n",
    "    Returns:\n",
    "    list: A list containing all GitHub repo urls associated with the keyword.\n",
    "  \n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    page = None\n",
    "    for page_num in tqdm(range(start, stop), desc=\"Scraping top GitHub URLs...\"):\n",
    "        keyword_no_space = (\"+\").join(keyword.split(\" \"))\n",
    "        url = f\"https://github.com/search?o=desc&p={str(page_num)}&q={keyword_no_space}&s=&type={TYPE}\"\n",
    "        page = requests.get(url, headers={'User-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15'})\n",
    "        while page.status_code == 429:\n",
    "            # Check that the page was fetched, otherwise time out and retry\n",
    "            print(\"Timeout, retrying to fetch repo urls...\")\n",
    "            time.sleep(60)\n",
    "            page = requests.get(url, headers={'User-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15'})\n",
    "        # Extract contents\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        a_tags = soup.find_all(\"a\", class_=SCRAPE_CLASS[TYPE])\n",
    "        new_urls = [a_tag.get(\"href\") for a_tag in a_tags]\n",
    "        # Gather the relevant urls\n",
    "        urls.extend(new_urls)\n",
    "        time.sleep(5)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca64550",
   "metadata": {},
   "source": [
    "##### GitHub Repo Contributor Scraping Functions\n",
    "Now we'll scrape the information about the top ten contributors of each repo, such that we can later combine the information to illustrate connections between different repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_contributors(repo_url, repo_contributor_rel, repo_id, contributors_set, n_contributors=10):\n",
    "    \"\"\"\n",
    "    Scrapes the top 'n_contributors' of the repo given under 'repo_url' and saves the mapping into \n",
    "    'repo_contributor_rel' with the given 'repo_id' and returns a new dataframe with all the \n",
    "    contributor information.\n",
    "  \n",
    "    Parameters:\n",
    "    repo_url (string): repo to scrape contributors for\n",
    "    repo_contributor_rel (set): set to be populated with tuples of (repo_id, contrib_username, # of contrib)\n",
    "    repo_id (int): the id of the GitHub repo\n",
    "    contributors_set (set): set of all already known contributors\n",
    "    n_contributors (int): number of ordered contributors to maximally scrape from a given repository. Default = 10.\n",
    "  \n",
    "    Returns:\n",
    "    dataframe: A df containing all the information for the top n contributors of the given\n",
    "               GitHub repo.\n",
    "\n",
    "    Throws:\n",
    "    an error for status code 403 whilst fetching the contributors page\n",
    "  \n",
    "    \"\"\"\n",
    "    contributor_url = (f\"https://api.github.com/repos{repo_url}/contributors\")\n",
    "    contributor_page = requests.get(contributor_url, auth=(USERNAME, TOKEN))\n",
    "    all_contributors = dict()\n",
    "    while contributor_page.status_code == 403:\n",
    "        print('Sleeping')\n",
    "        time.sleep(1500)\n",
    "        contributor_page = requests.get(contributor_url, auth=(USERNAME, TOKEN))\n",
    "    if contributor_page.status_code != 204:\n",
    "        if contributor_page.status_code == 403:\n",
    "            # If this happens, restart the script\n",
    "            raise Exception('This is the exception you expect to handle')\n",
    "        contributor_page = contributor_page.json()\n",
    "        max_n_top_contributors = min(len(contributor_page),n_contributors)\n",
    "\n",
    "        profile=None\n",
    "        profile_features = [\"login\", \"url\", \"type\", \"name\", \"company\", \"location\", \"hireable\", \"bio\", \"public_repos\", \"public_gists\", \"followers\", \"following\", \"created_at\"]\n",
    "        if max_n_top_contributors > 0 and type(contributor_page) == list:\n",
    "            for n in range(max_n_top_contributors):\n",
    "                contributor = contributor_page[n]\n",
    "                # Add an entry into the repo_contributor_relation df consisting of the repo id, the contributor username and the amount of contributions of that user on this repo\n",
    "                repo_contributor_rel.add((repo_id, contributor[\"login\"], contributor[\"contributions\"]))\n",
    "                if contributor[\"login\"] not in contributors_set and contributor[\"contributions\"] > 10:\n",
    "                    # Save all users with a significant amount of contributions to any repo, but only do it once\n",
    "                    contributors_set.add(contributor[\"login\"])\n",
    "                    profile = requests.get(contributor[\"url\"], auth=(USERNAME, TOKEN),headers=headers)\n",
    "                    while profile.status_code == 429:\n",
    "                        # Check for timeouts and retry again after waiting\n",
    "                        print(\"Timeout, retrying to fetch contributor profile...\")\n",
    "                        time.sleep(30)\n",
    "                        profile = requests.get(contributor[\"url\"], auth=(USERNAME, TOKEN),headers=headers)\n",
    "                    all_contributors[contributor[\"login\"]] = {key: val for key, val in profile.json().items() if key in profile_features}\n",
    "    return pd.DataFrame.from_dict(all_contributors,orient='index')\n",
    "\n",
    "def get_all_contributors(repos,repo_contributor_rel,contributors_set,n_contributors=10):\n",
    "    \"\"\"\n",
    "    Scrapes the top 'n_contributors' of all the 'repos' given and saves the mapping into \n",
    "    'repo_contributor_rel' onto disk as 'most_updated_(n_contributors)_contributor_info_stop75.csv'. \n",
    "    Also saves a new dataframe with all the contributor information as \n",
    "    'repo_contributor_relationship_table_stop75.csv' to disk.\n",
    "  \n",
    "    Parameters:\n",
    "    repos (list): tuples of (url, repo_id) pairs for all the repos to scrape contributors for\n",
    "    repo_contributor_rel (set): set to be populated with tuples of (repo_id, contrib_username, # of contrib)\n",
    "    contributors_set (set): set of all already known contributors\n",
    "    n_contributors (int): number of ordered contributors to maximally scrape from a given repository. Default = 10.\n",
    "  \n",
    "    Returns:\n",
    "    nothing\n",
    "\n",
    "    Throws:\n",
    "    an error for status code 403 whilst fetching the contributors page\n",
    "  \n",
    "    \"\"\"\n",
    "    contributor_df = pd.DataFrame(columns=[\"login\", \"url\", \"type\", \"name\", \"company\", \"location\", \"hireable\", \"bio\", \"public_repos\", \"public_gists\", \"followers\", \"following\", \"created_at\"])\n",
    "    repos_zip = list(zip(repos.url,repos.id))\n",
    "    for url, r_id in tqdm(repos_zip, desc=\"Scraping top contributors info...\"):\n",
    "        # Get top n_contributors of this repo\n",
    "        new_contributors = get_repo_contributors(url, repo_contributor_rel, r_id, contributors_set, n_contributors=n_contributors)\n",
    "        # print(url,len(new_contributors.index))\n",
    "        # Add it to the already scraped ones\n",
    "        contributor_df = pd.concat([contributor_df, new_contributors]).drop_duplicates()\n",
    "        # Save progress of contributor data to disk -> Change name if you want to\n",
    "        contributor_df.to_csv(f'data/most_updated_{n_contributors}_contributor_info_stop75.csv')\n",
    "        # Save relationship df to disk -> Change name if you want to\n",
    "        pd.DataFrame(repo_contributor_rel, columns=['Repo','Contributor','Contributions']).sort_values('Repo').to_csv('data/repo_contributor_relationship_table_stop75.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282960f8",
   "metadata": {},
   "source": [
    "### Scraping the data: Example function calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get repo information based on the search words\n",
    "start = 0\n",
    "stop = 75\n",
    "\n",
    "repos = all_repo_info(keywords, start=start, stop=stop)\n",
    "topic_rel = topic_relationship_table(repos, output_file = f'data/topic_relationship_table_stop{stop}')\n",
    "\n",
    "#Next get contributor information for all repositories previously scraped\n",
    "\n",
    "contributors = set()\n",
    "repo_contributor_rel = set()\n",
    "get_all_contributors(df.read_csv(f'data/most_updated_repo_info_stop{start}to{stop}.csv'),repo_contributor_rel,contributors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
